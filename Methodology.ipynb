{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "941yGym4goeh"
      },
      "source": [
        "Steps Overview:\n",
        "1. Load DICOM files and metadata files (CSV)\n",
        "2. Preprocess the DICOM images\n",
        "3. Merge metadata from CSVs\n",
        "4. Prepare data for modeling (resize images, normalize, etc.)\n",
        "5. Build a model for classification\n",
        "6. Train and evaluate the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTvBkdJnhOVS"
      },
      "source": [
        "\n",
        "1. Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrotD2BelPp1",
        "outputId": "7e35aaa8-9ee7-4a7f-bcd4-34874a42ebc8"
      },
      "outputs": [],
      "source": [
        "!pip install pylibjpeg pylibjpeg-libjpeg pylibjpeg-openjpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcHcV9YZlUDi",
        "outputId": "f0278163-f168-4235-875a-81ad83544c5f"
      },
      "outputs": [],
      "source": [
        "!pip install pydicom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XDmrrrePgoAE",
        "outputId": "3387dd5a-5ed2-4dfd-9f34-ca887a43fe29"
      },
      "outputs": [],
      "source": [
        "!pip install pydicom matplotlib pandas tensorflow scikit-learn\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install pydicom SimpleITK matplotlib kaggle\n",
        "\n",
        "# Set up Kaggle authentication\n",
        "from google.colab import files\n",
        "files.upload()  # Upload kaggle.json file\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download the RSNA dataset\n",
        "!kaggle competitions download -c rsna-2024-lumbar-spine-degenerative-classification\n",
        "!unzip -q rsna-2024-lumbar-spine-degenerative-classification.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2awOAXYhSwF"
      },
      "source": [
        "2. Load Metadata Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aNXTH6-hIpw",
        "outputId": "642a12e2-1f29-40a9-a8e2-ab4dbd2c8017"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the metadata CSVs\n",
        "train_labels = pd.read_csv('/content/train_label_coordinates.csv')\n",
        "train_data = pd.read_csv('/content/train.csv')\n",
        "train_series_descriptions = pd.read_csv('/content/train_series_descriptions.csv')\n",
        "\n",
        "# Display the first few rows of each file to check their structure\n",
        "print(\"Train Labels:\\n\", train_labels.head())\n",
        "print(\"Train Data:\\n\", train_data.head())\n",
        "print(\"Train Series Descriptions:\\n\", train_series_descriptions.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIR8IQPblATF"
      },
      "source": [
        "Load DICOM Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pszWGjqjlC_c",
        "outputId": "22f13188-c8bf-426d-c7f2-ee3972410cd1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pydicom\n",
        "\n",
        "# Set the root directory for DICOM images\n",
        "data_path = \"/content/train_images\"\n",
        "\n",
        "# Recursively find all DICOM files\n",
        "dicom_files = []\n",
        "for dirpath, _, filenames in os.walk(data_path):\n",
        "    for f in filenames:\n",
        "        if f.endswith(\".dcm\"):\n",
        "            dicom_files.append(os.path.join(dirpath, f))\n",
        "\n",
        "# Check the number of DICOM files found\n",
        "print(f\"Number of DICOM files found: {len(dicom_files)}\")\n",
        "\n",
        "# Load and visualize a DICOM image\n",
        "first_dicom = pydicom.dcmread(dicom_files[0])\n",
        "print(first_dicom)\n",
        "\n",
        "# Visualize the image\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(first_dicom.pixel_array, cmap='gray')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiVz88v-ldYc"
      },
      "source": [
        "4. Preprocess Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gPTX-vjXleB0"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Define image size\n",
        "IMG_SIZE = 224  # You can modify this size based on model input requirements\n",
        "\n",
        "# Function to resize and normalize DICOM image\n",
        "def preprocess_dicom(dicom_path):\n",
        "    dicom_image = pydicom.dcmread(dicom_path)\n",
        "    pixel_array = dicom_image.pixel_array\n",
        "\n",
        "    # Resize image to a fixed size\n",
        "    resized_image = cv2.resize(pixel_array, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "    # Normalize image (scale pixel values to [0, 1])\n",
        "    normalized_image = resized_image / np.max(resized_image)\n",
        "\n",
        "    return normalized_image\n",
        "\n",
        "# Preprocess all DICOM files\n",
        "processed_images = [preprocess_dicom(file) for file in dicom_files[:100]]  # Limit to first 100 images for now\n",
        "\n",
        "# Convert to numpy array for modeling\n",
        "X = np.array(processed_images)\n",
        "X = X.reshape(X.shape[0], IMG_SIZE, IMG_SIZE, 1)  # Add channel dimension\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMA0BBHnln5G"
      },
      "source": [
        "5. Merge Labels with DICOM Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzW0-vYYxmjB",
        "outputId": "b2fda8bd-6b92-4865-f816-7553495bf788"
      },
      "outputs": [],
      "source": [
        "import pydicom\n",
        "import os\n",
        "\n",
        "# List to store extracted metadata\n",
        "dicom_metadata = []\n",
        "\n",
        "# Loop through DICOM files and extract relevant metadata\n",
        "for dicom_file in dicom_files[:100]:  # Adjust the range to handle all files if needed\n",
        "    try:\n",
        "        # Read the DICOM file\n",
        "        dicom_data = pydicom.dcmread(dicom_file)\n",
        "\n",
        "        # Extract relevant metadata fields\n",
        "        study_id = dicom_data.PatientID  # Corresponds to 'study_id'\n",
        "        series_id = dicom_data.SeriesInstanceUID  # Corresponds to 'series_id'\n",
        "        instance_number = dicom_data.InstanceNumber  # Corresponds to 'instance_number'\n",
        "\n",
        "        # Append to the metadata list\n",
        "        dicom_metadata.append({\n",
        "            'study_id': study_id,\n",
        "            'series_id': series_id,\n",
        "            'instance_number': instance_number,\n",
        "            'file_path': dicom_file\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file {dicom_file}: {e}\")\n",
        "\n",
        "# Convert to a DataFrame for easier merging\n",
        "dicom_df = pd.DataFrame(dicom_metadata)\n",
        "\n",
        "# Display the extracted DICOM metadata\n",
        "print(dicom_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "t88bCyTUyR6k"
      },
      "outputs": [],
      "source": [
        "# If column names are different, rename them for consistency\n",
        "dicom_df = dicom_df.rename(columns={\"SeriesInstanceUID\": \"series_id\"})  # Adjust if necessary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "6539TJbTzdJf"
      },
      "outputs": [],
      "source": [
        "# Load the metadata CSVs\n",
        "train_labels = pd.read_csv('/content/train_label_coordinates.csv')\n",
        "train_data = pd.read_csv('/content/train.csv')\n",
        "train_series_descriptions = pd.read_csv('/content/train_series_descriptions.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "kjlqwnteztzJ"
      },
      "outputs": [],
      "source": [
        "# ## naming the new dataframes of the csv files\n",
        "\n",
        "# train1 = pd.read_csv('/content/train.csv')\n",
        "# traincor2 = pd.read_csv('/content/train_label_coordinates.csv')\n",
        "# trainseries3 = pd.read_csv('/content/train_series_descriptions.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "l3X88n3gz7AV"
      },
      "outputs": [],
      "source": [
        "merged_df = pd.merge(train_labels, train_series_descriptions, on='series_id', how='inner')\n",
        "# Assuming 'study_id' is identical in both DataFrames\n",
        "merged_df['study_id'] = merged_df['study_id_x']  # or merged_df['study_id_y'], depending on preference\n",
        "\n",
        "# Drop the redundant 'study_id_x' and 'study_id_y' columns\n",
        "merged_df = merged_df.drop(columns=['study_id_x', 'study_id_y'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 846
        },
        "id": "PB9YX-zl0Ffu",
        "outputId": "2280634a-1106-4f6a-bc13-bddb74c4ec0c"
      },
      "outputs": [],
      "source": [
        "final_merged_df = pd.merge(merged_df, train_data, on='study_id', how='inner')\n",
        "final_merged_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwaqDP9R0O6H",
        "outputId": "287f99fb-048f-42eb-e8d4-d7de22d80a74"
      },
      "outputs": [],
      "source": [
        "final_merged_df.columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkPZRKdf0aqE",
        "outputId": "f75c4053-7341-4f84-e75f-6c6ef3b26949"
      },
      "outputs": [],
      "source": [
        "len(final_merged_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GW2ywM1vzc6B",
        "outputId": "5b3d2983-0090-4444-9c76-7b1462a8f298"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pydicom\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "# Function to map the level to the corresponding stenosis column in the DataFrame\n",
        "def get_stenosis_column(level, condition):\n",
        "    stenosis_map = {\n",
        "        'L1/L2': {\n",
        "            'Spinal Canal Stenosis': 'spinal_canal_stenosis_l1_l2',\n",
        "            'Left Neural Foraminal Narrowing': 'left_neural_foraminal_narrowing_l1_l2',\n",
        "            'Right Neural Foraminal Narrowing': 'right_neural_foraminal_narrowing_l1_l2',\n",
        "            'Left Subarticular Stenosis': 'left_subarticular_stenosis_l1_l2',\n",
        "            'Right Subarticular Stenosis': 'right_subarticular_stenosis_l1_l2',\n",
        "        },\n",
        "        'L2/L3': {\n",
        "            'Spinal Canal Stenosis': 'spinal_canal_stenosis_l2_l3',\n",
        "            'Left Neural Foraminal Narrowing': 'left_neural_foraminal_narrowing_l2_l3',\n",
        "            'Right Neural Foraminal Narrowing': 'right_neural_foraminal_narrowing_l2_l3',\n",
        "            'Left Subarticular Stenosis': 'left_subarticular_stenosis_l2_l3',\n",
        "            'Right Subarticular Stenosis': 'right_subarticular_stenosis_l2_l3',\n",
        "        },\n",
        "        'L3/L4': {\n",
        "            'Spinal Canal Stenosis': 'spinal_canal_stenosis_l3_l4',\n",
        "            'Left Neural Foraminal Narrowing': 'left_neural_foraminal_narrowing_l3_l4',\n",
        "            'Right Neural Foraminal Narrowing': 'right_neural_foraminal_narrowing_l3_l4',\n",
        "            'Left Subarticular Stenosis': 'left_subarticular_stenosis_l3_l4',\n",
        "            'Right Subarticular Stenosis': 'right_subarticular_stenosis_l3_l4',\n",
        "        },\n",
        "        'L4/L5': {\n",
        "            'Spinal Canal Stenosis': 'spinal_canal_stenosis_l4_l5',\n",
        "            'Left Neural Foraminal Narrowing': 'left_neural_foraminal_narrowing_l4_l5',\n",
        "            'Right Neural Foraminal Narrowing': 'right_neural_foraminal_narrowing_l4_l5',\n",
        "            'Left Subarticular Stenosis': 'left_subarticular_stenosis_l4_l5',\n",
        "            'Right Subarticular Stenosis': 'right_subarticular_stenosis_l4_l5',\n",
        "        },\n",
        "        'L5/S1': {\n",
        "            'Spinal Canal Stenosis': 'spinal_canal_stenosis_l5_s1',\n",
        "            'Left Neural Foraminal Narrowing': 'left_neural_foraminal_narrowing_l5_s1',\n",
        "            'Right Neural Foraminal Narrowing': 'right_neural_foraminal_narrowing_l5_s1',\n",
        "            'Left Subarticular Stenosis': 'left_subarticular_stenosis_l5_s1',\n",
        "            'Right Subarticular Stenosis': 'right_subarticular_stenosis_l5_s1',\n",
        "        }\n",
        "    }\n",
        "    return stenosis_map.get(level, {}).get(condition, None)\n",
        "\n",
        "# Function to apply CLAHE to the image\n",
        "def apply_clahe(image):\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    enhanced_image = clahe.apply(image)\n",
        "    return enhanced_image\n",
        "\n",
        "# Function to normalize the image data to 8-bit format (uint8)\n",
        "def normalize_to_uint8(image):\n",
        "    image_normalized = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX)\n",
        "    return image_normalized.astype(np.uint8)\n",
        "\n",
        "# Function to color code the severity and normalize it for matplotlib\n",
        "def get_circle_color(severity):\n",
        "    severity_color_map = {\n",
        "        'Normal/Mild': (0, 1.0, 0),    # Green\n",
        "        'Moderate': (1.0, 1.0, 0),     # Yellow\n",
        "        'Severe': (1.0, 0, 0)          # Red\n",
        "    }\n",
        "    return severity_color_map.get(severity, (1.0, 1.0, 1.0))  # Default to white if unknown\n",
        "\n",
        "# Function to display images grouped by description with hollow dots for severity\n",
        "def display_images_grouped_by_description(images_by_description, study_id):\n",
        "    for description, images in images_by_description.items():\n",
        "        num_images = len(images)\n",
        "        fig, axs = plt.subplots(1, num_images, figsize=(20, 6))\n",
        "\n",
        "        if num_images == 1:\n",
        "            axs = [axs]  # To ensure axs is iterable even for a single image\n",
        "\n",
        "        for i, (img, instance_number, x, y, severity, series_id) in enumerate(images):\n",
        "            enhanced_img = apply_clahe(img)\n",
        "            axs[i].imshow(enhanced_img, cmap='gray')\n",
        "            axs[i].scatter([x], [y], facecolors='none', edgecolors=get_circle_color(severity), s=100, linewidths=2)\n",
        "            axs[i].set_title(f\"{instance_number}.dcm\")\n",
        "            axs[i].axis('off')\n",
        "\n",
        "        plt.suptitle(f\"Study ID: {study_id} | Series ID: {series_id} | Description: {description}\", fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Main function to process images and categorize them by description\n",
        "def process_study_ids_by_description(df, root_dir):\n",
        "    for study_id in df['study_id'].unique():\n",
        "        study_df = df[df['study_id'] == study_id]\n",
        "        images_by_description = {}\n",
        "\n",
        "        for series_id in study_df['series_id'].unique():\n",
        "            series_df = study_df[study_df['series_id'] == series_id]\n",
        "            series_description = series_df['series_description'].iloc[0]\n",
        "\n",
        "            study_folder = os.path.join(root_dir, str(study_id))\n",
        "            series_folder = os.path.join(study_folder, str(series_id))\n",
        "\n",
        "            if not os.path.exists(series_folder):\n",
        "                print(f\"Series folder {series_folder} not found.\")\n",
        "                continue\n",
        "\n",
        "            for index, row in series_df.iterrows():\n",
        "                instance_number = int(row['instance_number'])\n",
        "                x = row['x']\n",
        "                y = row['y']\n",
        "                level = row['level']\n",
        "                condition = row['condition']\n",
        "\n",
        "                stenosis_column = get_stenosis_column(level, condition)\n",
        "                if stenosis_column is None:\n",
        "                    print(f\"No corresponding stenosis column found for level {level} and condition {condition}.\")\n",
        "                    continue\n",
        "\n",
        "                condition_status = row[stenosis_column]\n",
        "                dcm_file = f\"{instance_number}.dcm\"\n",
        "                dcm_path = os.path.join(series_folder, dcm_file)\n",
        "\n",
        "                if not os.path.exists(dcm_path):\n",
        "                    print(f\"DICOM file {dcm_file} not found for Series ID: {series_id}.\")\n",
        "                    continue\n",
        "\n",
        "                dicom_data = pydicom.dcmread(dcm_path)\n",
        "                image = normalize_to_uint8(dicom_data.pixel_array)\n",
        "\n",
        "                if series_description not in images_by_description:\n",
        "                    images_by_description[series_description] = []\n",
        "                images_by_description[series_description].append((image, instance_number, x, y, condition_status, series_id))\n",
        "\n",
        "        display_images_grouped_by_description(images_by_description, study_id)\n",
        "\n",
        "# Usage Example\n",
        "root_dir = '/content/train_images'\n",
        "process_study_ids_by_description(final_merged_df, root_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-riJyyvY2xGW",
        "outputId": "af9c4243-2c0a-481e-d5c6-dcc2ad19dbea"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pydicom\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "import pandas as pd\n",
        "\n",
        "# Function to map the level to the corresponding stenosis column in the DataFrame\n",
        "def get_stenosis_column(level, condition):\n",
        "    stenosis_map = {\n",
        "        'L1/L2': {\n",
        "            'Spinal Canal Stenosis': 'spinal_canal_stenosis_l1_l2',\n",
        "            'Left Neural Foraminal Narrowing': 'left_neural_foraminal_narrowing_l1_l2',\n",
        "            'Right Neural Foraminal Narrowing': 'right_neural_foraminal_narrowing_l1_l2',\n",
        "            'Left Subarticular Stenosis': 'left_subarticular_stenosis_l1_l2',\n",
        "            'Right Subarticular Stenosis': 'right_subarticular_stenosis_l1_l2',\n",
        "        },\n",
        "        'L2/L3': {\n",
        "            'Spinal Canal Stenosis': 'spinal_canal_stenosis_l2_l3',\n",
        "            'Left Neural Foraminal Narrowing': 'left_neural_foraminal_narrowing_l2_l3',\n",
        "            'Right Neural Foraminal Narrowing': 'right_neural_foraminal_narrowing_l2_l3',\n",
        "            'Left Subarticular Stenosis': 'left_subarticular_stenosis_l2_l3',\n",
        "            'Right Subarticular Stenosis': 'right_subarticular_stenosis_l2_l3',\n",
        "        },\n",
        "        'L3/L4': {\n",
        "            'Spinal Canal Stenosis': 'spinal_canal_stenosis_l3_l4',\n",
        "            'Left Neural Foraminal Narrowing': 'left_neural_foraminal_narrowing_l3_l4',\n",
        "            'Right Neural Foraminal Narrowing': 'right_neural_foraminal_narrowing_l3_l4',\n",
        "            'Left Subarticular Stenosis': 'left_subarticular_stenosis_l3_l4',\n",
        "            'Right Subarticular Stenosis': 'right_subarticular_stenosis_l3_l4',\n",
        "        },\n",
        "        'L4/L5': {\n",
        "            'Spinal Canal Stenosis': 'spinal_canal_stenosis_l4_l5',\n",
        "            'Left Neural Foraminal Narrowing': 'left_neural_foraminal_narrowing_l4_l5',\n",
        "            'Right Neural Foraminal Narrowing': 'right_neural_foraminal_narrowing_l4_l5',\n",
        "            'Left Subarticular Stenosis': 'left_subarticular_stenosis_l4_l5',\n",
        "            'Right Subarticular Stenosis': 'right_subarticular_stenosis_l4_l5',\n",
        "        },\n",
        "        'L5/S1': {\n",
        "            'Spinal Canal Stenosis': 'spinal_canal_stenosis_l5_s1',\n",
        "            'Left Neural Foraminal Narrowing': 'left_neural_foraminal_narrowing_l5_s1',\n",
        "            'Right Neural Foraminal Narrowing': 'right_neural_foraminal_narrowing_l5_s1',\n",
        "            'Left Subarticular Stenosis': 'left_subarticular_stenosis_l5_s1',\n",
        "            'Right Subarticular Stenosis': 'right_subarticular_stenosis_l5_s1',\n",
        "        }\n",
        "    }\n",
        "    return stenosis_map.get(level, {}).get(condition, None)\n",
        "\n",
        "# Function to apply CLAHE to the image\n",
        "def apply_clahe(image):\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    enhanced_image = clahe.apply(image)\n",
        "    return enhanced_image\n",
        "\n",
        "# Function to normalize the image data to 8-bit format (uint8)\n",
        "def normalize_to_uint8(image):\n",
        "    image_normalized = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX)\n",
        "    return image_normalized.astype(np.uint8)\n",
        "\n",
        "# Mapping severity labels to integers\n",
        "severity_map = {\n",
        "    'Normal/Mild': 0,\n",
        "    'Moderate': 1,\n",
        "    'Severe': 2\n",
        "}\n",
        "\n",
        "# Collect images and convert severity labels to integers using the severity map\n",
        "def collect_images_and_labels(df, root_dir):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for study_id in df['study_id'].unique():\n",
        "        study_df = df[df['study_id'] == study_id]\n",
        "\n",
        "        for series_id in study_df['series_id'].unique():\n",
        "            series_df = study_df[study_df['series_id'] == series_id]\n",
        "\n",
        "            study_folder = os.path.join(root_dir, str(study_id))\n",
        "            series_folder = os.path.join(study_folder, str(series_id))\n",
        "\n",
        "            if not os.path.exists(series_folder):\n",
        "                print(f\"Series folder {series_folder} not found.\")\n",
        "                continue\n",
        "\n",
        "            for index, row in series_df.iterrows():\n",
        "                instance_number = int(row['instance_number'])\n",
        "                level = row['level']\n",
        "                condition = row['condition']\n",
        "\n",
        "                # Skip rows with missing condition or NaN\n",
        "                stenosis_column = get_stenosis_column(level, condition)\n",
        "                if stenosis_column is None:\n",
        "                    print(f\"No corresponding stenosis column found for level {level} and condition {condition}.\")\n",
        "                    continue\n",
        "\n",
        "                condition_status = row.get(stenosis_column, None)\n",
        "                if pd.isna(condition_status):\n",
        "                    print(f\"Skipping row with missing condition status for study {study_id}, series {series_id}.\")\n",
        "                    continue\n",
        "\n",
        "                dcm_file = f\"{instance_number}.dcm\"\n",
        "                dcm_path = os.path.join(series_folder, dcm_file)\n",
        "\n",
        "                if not os.path.exists(dcm_path):\n",
        "                    print(f\"DICOM file {dcm_file} not found for Series ID: {series_id}.\")\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    dicom_data = pydicom.dcmread(dcm_path)\n",
        "                    image = normalize_to_uint8(dicom_data.pixel_array)\n",
        "                    image = apply_clahe(image)  # Apply CLAHE\n",
        "                    image = cv2.resize(image, (256, 256))  # Resize for CNN\n",
        "\n",
        "                    # Append image and label (convert severity to integer)\n",
        "                    images.append(np.expand_dims(image, axis=-1))  # Add channel dimension\n",
        "                    labels.append(severity_map[condition_status])  # Convert label to integer\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error reading or processing DICOM file {dcm_file}: {e}\")\n",
        "                    continue\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Build the CNN model\n",
        "def build_cnn_model():\n",
        "    model = Sequential()\n",
        "\n",
        "    # First convolutional layer\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 1)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Second convolutional layer\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Third convolutional layer\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Flatten and fully connected layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(3, activation='softmax'))  # 3 classes for severity\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Load data\n",
        "root_dir = '/content/train_images'  # Replace with your actual directory\n",
        "images, labels = collect_images_and_labels(final_merged_df, root_dir)\n",
        "\n",
        "# Convert labels to categorical format\n",
        "labels_categorical = to_categorical(labels, num_classes=3)\n",
        "\n",
        "# Build the model\n",
        "cnn_model = build_cnn_model()\n",
        "\n",
        "# Train the CNN model\n",
        "cnn_model.fit(images, labels_categorical, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Save the trained model\n",
        "cnn_model.save('cnn_stenosis_model.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD9Q6AoaseZl"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3A1QcWQmBAW"
      },
      "source": [
        "6. Build a CNN Model for Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "dEDdmGUSmBb_",
        "outputId": "e31c05ab-ebfe-4776-f340-ccbcb0477f7c"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add convolutional layers\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flatten and add dense layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))  # For binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHRBgjbImNhv"
      },
      "source": [
        "7. Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVcWt_P2Q41m",
        "outputId": "bbcc9e7a-6c1d-4566-9166-10a3e9afd612"
      },
      "outputs": [],
      "source": [
        "!pip install pydicom\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsqnBilQTmHj",
        "outputId": "b6d2906b-da6c-4c31-d9c9-2337b9aeb982"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-addons\n",
        "!pip install tensorflow keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_A7Hf5zU5Vk",
        "outputId": "3a8826fd-5062-417e-9c75-69904ebea3e4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m3WLqCyQOjgf",
        "outputId": "404da145-e550-43b4-b453-e68039ce56c5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pydicom\n",
        "import cv2\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from skimage.transform import resize\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from matplotlib.patches import Rectangle\n",
        "import warnings\n",
        "#import tensorflow_addons as tfa\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "# Paths\n",
        "train_path = '/content/train.csv'\n",
        "train_label_coordinates_path = '/content/train_label_coordinates.csv'\n",
        "train_series_descriptions_path = '/content/train_series_descriptions.csv'\n",
        "test_series_descriptions_path = '/content/test_series_descriptions.csv'\n",
        "sample_submission_path = '/content/sample_submission.csv'\n",
        "\n",
        "# Load data\n",
        "df_train = pd.read_csv(train_path)\n",
        "df_train_label_coordinates = pd.read_csv(train_label_coordinates_path)\n",
        "df_train_series_descriptions = pd.read_csv(train_series_descriptions_path)\n",
        "df_test_series_descriptions = pd.read_csv(test_series_descriptions_path)\n",
        "df_sample_submission = pd.read_csv(sample_submission_path)\n",
        "# Preprocess data\n",
        "df_train = df_train.dropna()\n",
        "df_train_label_coordinates = df_train_label_coordinates.dropna()\n",
        "df_train_series_descriptions = df_train_series_descriptions.dropna()\n",
        "df_test_series_descriptions = df_test_series_descriptions.dropna()\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_train = df_train.drop('Unnamed: 0', axis=1, errors='ignore')\n",
        "df_train_label_coordinates = df_train_label_coordinates.drop('Unnamed: 0', axis=1, errors='ignore')\n",
        "df_train_series_descriptions = df_train_series_descriptions.drop('Unnamed: 0', axis=1, errors='ignore')\n",
        "df_test_series_descriptions = df_test_series_descriptions.drop('Unnamed: 0', axis=1, errors='ignore')\n",
        "# Encode labels\n",
        "le_condition = LabelEncoder()\n",
        "df_train_label_coordinates['condition'] = le_condition.fit_transform(df_train_label_coordinates['condition'])\n",
        "\n",
        "le_level = LabelEncoder()\n",
        "df_train_label_coordinates['level'] = le_level.fit_transform(df_train_label_coordinates['level'])\n",
        "\n",
        "le_target = LabelEncoder()\n",
        "df_train_label_coordinates['target'] = le_target.fit_transform(df_train_label_coordinates['condition'])\n",
        "# Function to load DICOM images\n",
        "# Function to load DICOM images\n",
        "def load_dicom_image(path):\n",
        "    dicom = pydicom.dcmread(path)  # Replaced read_file with dcmread\n",
        "    image = dicom.pixel_array\n",
        "    if image.dtype != np.uint8:\n",
        "        image = image.astype(np.uint8)\n",
        "    return image\n",
        "\n",
        "# Function to extract region\n",
        "def extract_region(image, x, y, width=128, height=128):\n",
        "  start_x = int(x - width / 2)\n",
        "  end_x = int(x + width / 2)\n",
        "  start_y = int(y - height / 2)\n",
        "  end_y = int(y + height / 2)\n",
        "\n",
        "  start_x = max(0, start_x)\n",
        "  end_x = min(image.shape[1], end_x)\n",
        "  start_y = max(0, start_y)\n",
        "  end_y = min(image.shape[0], end_y)\n",
        "\n",
        "  region = image[start_y:end_y, start_x:end_x]\n",
        "\n",
        "  if region.size == 0:\n",
        "      raise ValueError(\"Extracted region is empty. Please check the coordinates and image dimensions.\")\n",
        "\n",
        "  region = cv2.resize(region, (128, 128))\n",
        "  return region\n",
        "# Function to draw rectangle\n",
        "def draw_rectangle(image, x_coord, y_coord, size, color, label):\n",
        "  fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "  ax[0].imshow(image, cmap='gray')\n",
        "  ax[0].set_title('Original Image')\n",
        "\n",
        "  window_size = int(0.2 * min(image.shape))  # Adaptive window size (20%)\n",
        "  selected_area = image[max(0, int(y_coord) - window_size // 2):min(image.shape[0], int(y_coord) + window_size // 2),\n",
        "                        max(0, int(x_coord) - window_size // 2):min(image.shape[1], int(x_coord) + window_size // 2)]\n",
        "\n",
        "  ax[1].imshow(selected_area, cmap='gray')\n",
        "  rect = Rectangle((window_size // 2 - size // 2, window_size // 2 - size // 2), size, size, linewidth=2, edgecolor=color, facecolor='none')\n",
        "  ax[1].add_patch(rect)\n",
        "  ax[1].set_title(f'{label} Area at ({x_coord:.2f}, {y_coord:.2f})')\n",
        "\n",
        "  plt.show()\n",
        "# Function to draw severity\n",
        "def draw_severe(image, x_coord, y_coord, severity):\n",
        "  colors = [(1, 1, 0), (1, 0.5, 0), (1, 0, 0)]  # Yellow to Red\n",
        "  cmap = LinearSegmentedColormap.from_list(\"severity_cmap\", colors, N=3)\n",
        "\n",
        "  severity_level = le_target.inverse_transform([severity])[0]\n",
        "  severity_levels = le_target.classes_\n",
        "\n",
        "  if severity_level not in severity_levels:\n",
        "      raise ValueError(f\"Unexpected severity level: {severity_level}\")\n",
        "\n",
        "  severity_index = list(severity_levels).index(severity_level)\n",
        "  color = cmap(severity_index)\n",
        "\n",
        "  clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "  image = clahe.apply(image)\n",
        "\n",
        "  draw_rectangle(image, x_coord, y_coord, 24, color, severity)\n",
        "\n",
        "  extracted_region = extract_region(image, x_coord, y_coord, width=16, height=16)\n",
        "  return extracted_region\n",
        "# Function to load images from study\n",
        "def load_images_from_study(df, folder):\n",
        "  base_folder = folder\n",
        "  images = []\n",
        "  conditions = []\n",
        "  levels = []\n",
        "  targets = []\n",
        "  for _, row in df.iterrows():\n",
        "      study_id = row['study_id']\n",
        "      series_id = row['series_id']\n",
        "      x = row['x']\n",
        "      y = row['y']\n",
        "      condition = row['condition']\n",
        "      level = row['level']\n",
        "      target = row['target']\n",
        "\n",
        "      dicom_folder = f\"{base_folder}/{study_id}/{series_id}\"\n",
        "\n",
        "      if os.path.isdir(dicom_folder):\n",
        "          for dicom_file in os.listdir(dicom_folder):\n",
        "              dicom_path = os.path.join(dicom_folder, dicom_file)\n",
        "              img = load_dicom_image(dicom_path)\n",
        "              if img is not None:\n",
        "                  extracted_region = draw_severe(img, x, y, int(target))\n",
        "                  region = extract_region(img, x, y)\n",
        "                  images.append(region)\n",
        "                  conditions.append(condition)\n",
        "                  levels.append(level)\n",
        "                  targets.append(target)\n",
        "              else:\n",
        "                  print(f\"Failed to load image for {dicom_path}\")\n",
        "      else:\n",
        "          print(f\"Folder not found: {dicom_folder}\")\n",
        "  return np.array(images), np.array(conditions), np.array(levels), np.array(targets)\n",
        "# Prepare test data\n",
        "df_test = df_test_series_descriptions.merge(df_train_label_coordinates[['study_id', 'series_id', 'x', 'y', 'condition', 'level', 'target']], on=['study_id', 'series_id'], how='left')\n",
        "df_test = df_test.dropna(subset=['x', 'y', 'condition', 'level', 'target'])\n",
        "\n",
        "if df_test.empty:\n",
        "  df_test = pd.DataFrame({\n",
        "      'study_id': [44036939],\n",
        "      'series_id': [2828203845],\n",
        "      'x': [240],\n",
        "      'y': [120],\n",
        "      'condition': [0],\n",
        "      'level': [0],\n",
        "      'target': [0]\n",
        "  })\n",
        "\n",
        "X_test_images, X_test_conditions, X_test_levels, _ = load_images_from_study(df_test, '/content/test_images')\n",
        "\n",
        "if X_test_images.size == 0 or X_test_conditions.size == 0 or X_test_levels.size == 0:\n",
        "  raise ValueError(\"Error\")\n",
        "\n",
        "min_samples = min(X_test_images.shape[0], X_test_conditions.shape[0], X_test_levels.shape[0])\n",
        "\n",
        "X_test_images = X_test_images[:min_samples]\n",
        "X_test_conditions = X_test_conditions[:min_samples]\n",
        "X_test_levels = X_test_levels[:min_samples]\n",
        "\n",
        "X_test_images_resized = np.array([resize(image, (64, 64)) for image in X_test_images])\n",
        "X_test_images_resized = X_test_images_resized.reshape(-1, 64, 64, 1)\n",
        "\n",
        "X_test_conditions_encoded = to_categorical(X_test_conditions)\n",
        "X_test_levels_encoded = to_categorical(X_test_levels)\n",
        "# Create CNN model\n",
        "def create_cnn_model(input_shape):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(3, activation='softmax'))\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "input_shape = (64, 64, 1)\n",
        "model = create_cnn_model(input_shape)\n",
        "\n",
        "# # Create CNN model\n",
        "# def create_cnn_model(input_shape):\n",
        "#     model = Sequential()\n",
        "#     model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "#     model.add(MaxPooling2D((2, 2)))\n",
        "#     model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "#     model.add(MaxPooling2D((2, 2)))\n",
        "#     model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "#     model.add(MaxPooling2D((2, 2)))\n",
        "#     model.add(Flatten())\n",
        "#     model.add(Dense(128, activation='relu'))\n",
        "#     model.add(Dropout(0.5))\n",
        "#     model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "#     # Compile model with additional metrics\n",
        "#     model.compile(optimizer='adam',\n",
        "#                   loss='categorical_crossentropy',\n",
        "#                   metrics=['accuracy',\n",
        "#                            tfa.metrics.F1Score(num_classes=3, average='macro'),\n",
        "#                            tfa.metrics.Precision(),\n",
        "#                            tfa.metrics.Recall()])\n",
        "#     return model\n",
        "\n",
        "# input_shape = (64, 64, 1)\n",
        "# model = create_cnn_model(input_shape)\n",
        "\n",
        "\n",
        "# Dummy training data\n",
        "X_train_images = np.random.rand(10, 64, 64, 1)\n",
        "X_train_conditions = np.random.randint(0, 3, 10)\n",
        "X_train_levels = np.random.randint(0, 3, 10)\n",
        "y_train = to_categorical(np.random.randint(0, 3, 10))\n",
        "\n",
        "# Train model\n",
        "model.fit([X_train_images, to_categorical(X_train_conditions), to_categorical(X_train_levels)], y_train, epochs=3)\n",
        "\n",
        "# Load data\n",
        "df_train_data = pd.read_csv(train_path)\n",
        "df_train_labels = pd.read_csv(train_label_coordinates_path)\n",
        "df_train_series_desc = pd.read_csv(train_series_descriptions_path)\n",
        "df_test_series_desc = pd.read_csv(test_series_descriptions_path)\n",
        "df_submission_template = pd.read_csv(sample_submission_path)\n",
        "\n",
        "# Merge data\n",
        "df_merged_train_labels = pd.merge(left=df_train_labels, right=df_train_data, how='left', on='study_id').reset_index(drop=True)\n",
        "df_complete_train_data = pd.merge(left=df_merged_train_labels, right=df_train_series_desc, how='left', on=['study_id', 'series_id']).reset_index(drop=True)\n",
        "\n",
        "# Convert to category\n",
        "df_complete_train_data.study_id = df_complete_train_data.study_id.astype('category')\n",
        "df_complete_train_data.series_id = df_complete_train_data.series_id.astype('category')\n",
        "\n",
        "# Calculate frequencies\n",
        "label_columns = df_train_data.columns.drop('study_id').tolist()\n",
        "df_label_frequencies = pd.DataFrame(label_columns, columns=['label'])\n",
        "df_label_frequencies['p1'] = 1.0\n",
        "df_label_frequencies['p2'] = 0.0\n",
        "df_label_frequencies['p3'] = 0.0\n",
        "for label in label_columns:\n",
        "  relative_counts = df_train_data[label].value_counts(normalize=True)\n",
        "  df_label_frequencies.loc[df_label_frequencies.label == label, 'p1'] = relative_counts.get('Normal/Mild', 0)\n",
        "  df_label_frequencies.loc[df_label_frequencies.label == label, 'p2'] = relative_counts.get('Moderate', 0)\n",
        "  df_label_frequencies.loc[df_label_frequencies.label == label, 'p3'] = relative_counts.get('Severe', 0)\n",
        "\n",
        "# Frequency adjustment\n",
        "labels = df_train.columns.drop('study_id').tolist()\n",
        "freqs = pd.DataFrame(labels, columns=['label'])\n",
        "freqs['p1'] = 1.0\n",
        "freqs['p2'] = 0.0\n",
        "freqs['p3'] = 0.0\n",
        "for l in labels:\n",
        "  rel_counts = df_train[l].value_counts(normalize=True)\n",
        "  freqs.loc[freqs.label==l, 'p1'] = rel_counts.get('Normal/Mild', 0)\n",
        "  freqs.loc[freqs.label==l, 'p2'] = rel_counts.get('Moderate', 0)\n",
        "  freqs.loc[freqs.label==l, 'p3'] = rel_counts.get('Severe', 0)\n",
        "\n",
        "# Save train data\n",
        "df_complete_train_data.to_csv('complete_train_data.csv', index=False)\n",
        "\n",
        "# Explore specific study and series\n",
        "study_id = 100206310\n",
        "df_study_example = df_complete_train_data[df_complete_train_data.study_id == study_id]\n",
        "series_id = 1012284084\n",
        "df_series_example = df_study_example[df_study_example.series_id == series_id]\n",
        "# Explore DICOM files\n",
        "dicom_path = f'/content/train_images/{study_id}/{series_id}/'\n",
        "for dirname, _, filenames in os.walk(dicom_path):\n",
        "  for filename in filenames:\n",
        "      print(os.path.join(dirname, filename))\n",
        "\n",
        "for i in range(1, 10 + 1):\n",
        "  dicom_file = dicom_path + str(i) + '.dcm'\n",
        "  print(dicom_file)\n",
        "  ds = pydicom.dcmread(dicom_file)\n",
        "\n",
        "# Apply frequency adjustment to submission\n",
        "num_rows = df_submission_template.shape[0]\n",
        "for i in range(num_rows):\n",
        "  current_label = df_submission_template.loc[i, 'row_id'].split('_', 1)[1]\n",
        "  p1 = df_label_frequencies.loc[df_label_frequencies.label == current_label, 'p1'].min()\n",
        "  p2 = df_label_frequencies.loc[df_label_frequencies.label == current_label, 'p2'].min()\n",
        "  p3 = df_label_frequencies.loc[df_label_frequencies.label == current_label, 'p3'].min()\n",
        "  df_submission_template.loc[i, 'normal_mild'] = p1\n",
        "  df_submission_template.loc[i, 'moderate'] = p2\n",
        "  df_submission_template.loc[i, 'severe'] = p3\n",
        "\n",
        "# Save frequency-adjusted submission\n",
        "df_submission_template.to_csv('submission_with_frequencies.csv', index=False)\n",
        "\n",
        "# Apply uniform distribution to submission\n",
        "for i in range(num_rows):\n",
        "  df_submission_template.loc[i, 'normal_mild'] = 0.34\n",
        "  df_submission_template.loc[i, 'moderate'] = 0.33\n",
        "  df_submission_template.loc[i, 'severe'] = 0.33\n",
        "\n",
        "# Save uniform distribution submission\n",
        "df_submission_template.to_csv('submission.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJPYh0piUVwn",
        "outputId": "013b1dee-2804-425c-cee5-64e8340fb358"
      },
      "outputs": [],
      "source": [
        "# Assuming that your model is predicting conditions and levels combined, use conditions for simplicity\n",
        "y_test = X_test_conditions_encoded  # This is the encoded labels for conditions (as a placeholder for true labels)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_images_resized)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get predicted class labels\n",
        "y_true = np.argmax(y_test, axis=1)  # Get true class labels\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "precision = precision_score(y_true, y_pred_classes, average='macro')\n",
        "recall = recall_score(y_true, y_pred_classes, average='macro')\n",
        "f1 = f1_score(y_true, y_pred_classes, average='macro')\n",
        "\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
