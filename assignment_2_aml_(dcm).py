# -*- coding: utf-8 -*-
"""Assignment 2 AML (DCM).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qhPDbepnSjK5S4OXgBCX9G3J3qpE-fVv
"""

!pip install pydicom
!pip install gdcm

!pip install kaggle

from google.colab import files
files.upload()

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle competitions download -c rsna-2024-lumbar-spine-degenerative-classification

!unzip -q rsna-2024-lumbar-spine-degenerative-classification.zip

"""Loading and EDA

"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
from PIL import Image

# Load the CSV file (assuming it exists in the dataset)
labels_df = pd.read_csv('/content/train.csv')

# Display basic information and first few rows
print(labels_df.tail().info())
labels_df.tail()

from matplotlib import pyplot as plt
import seaborn as sns
_df_4.groupby('left_subarticular_stenosis_l4_l5').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

# Check for null values in each column
null_counts = labels_df.isnull().sum()

# Display the columns with their respective count of null values
print(null_counts[null_counts > 0])

import seaborn as sns
import matplotlib.pyplot as plt

# Visualize the null values with a heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(labels_df.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Values Heatmap')
plt.show()

# Calculate the percentage of missing values for each column
missing_percentage = (labels_df.isnull().sum() / len(labels_df)) * 100

# Filter columns with missing values
missing_percentage = missing_percentage[missing_percentage > 0]

# Plot the missing data
plt.figure(figsize=(10, 6))
missing_percentage.sort_values().plot(kind='barh', color='skyblue')
plt.title('Percentage of Missing Data by Column')
plt.xlabel('Percentage')
plt.ylabel('Column')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# # Define columns to visualize
# columns_to_visualize = [
#     'spinal_canal_stenosis_l1_l2', 'spinal_canal_stenosis_l2_l3',
#     'spinal_canal_stenosis_l3_l4', 'spinal_canal_stenosis_l4_l5',
#     'spinal_canal_stenosis_l5_s1', 'left_neural_foraminal_narrowing_l1_l2',
#     'left_neural_foraminal_narrowing_l2_l3', 'left_neural_foraminal_narrowing_l3_l4',
#     'left_neural_foraminal_narrowing_l4_l5', 'left_neural_foraminal_narrowing_l5_s1',
#     'right_neural_foraminal_narrowing_l1_l2', 'right_neural_foraminal_narrowing_l2_l3',
#     'right_neural_foraminal_narrowing_l3_l4', 'right_neural_foraminal_narrowing_l4_l5',
#     'right_neural_foraminal_narrowing_l5_s1', 'left_subarticular_stenosis_l1_l2',
#     'left_subarticular_stenosis_l2_l3', 'left_subarticular_stenosis_l3_l4',
#     'left_subarticular_stenosis_l4_l5', 'left_subarticular_stenosis_l5_s1',
#     'right_subarticular_stenosis_l1_l2', 'right_subarticular_stenosis_l2_l3',
#     'right_subarticular_stenosis_l3_l4', 'right_subarticular_stenosis_l4_l5',
#     'right_subarticular_stenosis_l5_s1'
# ]

# # Adjust figure size and layout
# plt.figure(figsize=(20, 25))
# num_cols = 4  # Number of columns in the grid

# # Loop through columns and create subplots
# for i, col in enumerate(columns_to_visualize, 1):
#     plt.subplot(len(columns_to_visualize) // num_cols + 1, num_cols, i)
#     sns.countplot(data=labels_df, x=col, palette='viridis')
#     plt.title(f'Distribution of {col}', fontsize=10)
#     plt.xlabel('')
#     plt.ylabel('Count')
#     plt.xticks(rotation=45)  # Rotate x-axis labels for better readability

# # Adjust layout to avoid overlapping
# plt.tight_layout()
# plt.show()


# Define the mapping from numerical labels to condition states
condition_mapping = {
    0: "No Stenosis",
    1: "Mild Stenosis",
    2: "Moderate Stenosis",
    3: "Severe Stenosis"
}

# Replace the numerical values in the DataFrame with the condition states
labels_df_mapped = labels_df.copy()
for col in columns_to_visualize:
    labels_df_mapped[col] = labels_df_mapped[col].replace(condition_mapping)

# Now you can visualize the class distribution using the mapped DataFrame
plt.figure(figsize=(20, 25))
num_cols = 4  # Number of columns in the grid

for i, col in enumerate(columns_to_visualize, 1):
    plt.subplot(len(columns_to_visualize) // num_cols + 1, num_cols, i)
    sns.countplot(data=labels_df_mapped, x=col, palette='viridis')
    plt.title(f'Distribution of {col}', fontsize=10)
    plt.xlabel('')
    plt.ylabel('Count')
    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability

plt.tight_layout()
plt.show()

import os
import matplotlib.pyplot as plt
import pydicom

# Path to the DICOM files folder (update this path according to your dataset structure)
dcm_folder = '/content/train_images/100206310/1012284084'

# List all DICOM files
dcm_files = []
for dirname, _, filenames in os.walk(dcm_folder):
    for filename in filenames:
        if filename.endswith('.dcm'):
            full_path = os.path.join(dirname, filename)
            dcm_files.append(full_path)

# Check if files are found
if not dcm_files:
    print("No DICOM files found in the specified directory.")
else:
    print(f"Total DICOM files found: {len(dcm_files)}")

# List to store the DICOM data
dicom_data_list = []

# Assuming dcm_files is a list of file paths
for dicom_file in dcm_files[:5]:  # Limit to the first 5 files
    try:
        dicom_data = pydicom.dcmread(dicom_file)
        if hasattr(dicom_data, 'pixel_array'):
            dicom_data_list.append((dicom_file, dicom_data))
        else:
            print(f"No pixel data in DICOM file: {dicom_file}")
    except Exception as e:
        print(f"Error reading {dicom_file}: {e}")

# Display the images and print all metadata
for dicom_file, dicom_data in dicom_data_list:
    try:
        print(f"Displaying DICOM file: {dicom_file}")
        plt.imshow(dicom_data.pixel_array, cmap=plt.cm.gray)
        plt.title(f"DICOM file: {dicom_file}")
        plt.axis('off')  # Hide axis labels
        plt.show()

        # Display all metadata
        print(f"Metadata for {dicom_file}:")
        for elem in dicom_data:
            print(f"{elem.tag} - {elem.name}: {elem.value}")

    except AttributeError:
        print(f"No pixel data found for {dicom_file}.")

import numpy as np
pixel_data = dicom_data.pixel_array
plt.hist(pixel_data.ravel(), bins=256, color='c')
plt.title('Pixel Intensity Distribution')
plt.xlabel('Pixel Intensity')
plt.ylabel('Frequency')
plt.show()

import cv2
import numpy as np
import pydicom
import matplotlib.pyplot as plt

# # Load the DICOM file
# dcm_file = "/path/to/your/dicom/file.dcm"  # Update with your file path
# dicom_data = pydicom.dcmread(dcm_file)

# Extract pixel array from the DICOM file
pixel_array = dicom_data.pixel_array

# Convert the pixel array to 8-bit (0-255) for histogram equalization
pixel_array_8bit = cv2.normalize(pixel_array, None, 0, 255, cv2.NORM_MINMAX).astype('uint8')

# Apply histogram equalization
equalized_image = cv2.equalizeHist(pixel_array_8bit)

# Display the original and equalized images side by side
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.imshow(pixel_array, cmap='gray')
plt.title('Original Image')
plt.axis('off')

plt.subplot(1, 2, 2)
plt.imshow(equalized_image, cmap='gray')
plt.title('Equalized Image')
plt.axis('off')

plt.show()

# Display the histograms of the original and equalized images
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.hist(pixel_array.ravel(), bins=256, color='c')
plt.title('Original Histogram')
plt.xlabel('Pixel Intensity')
plt.ylabel('Frequency')

plt.subplot(1, 2, 2)
plt.hist(equalized_image.ravel(), bins=256, color='c')
plt.title('Equalized Histogram')
plt.xlabel('Pixel Intensity')
plt.ylabel('Frequency')

plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import cv2
import pydicom
import numpy as np
import os
import glob
from tqdm import tqdm
import warnings
figure, axis = plt.subplots(1,3, figsize=(20,5))
for idx, d in enumerate(['foraminal', 'subarticular', 'canal']):
    diagnosis = list(filter(lambda x: x.find(d) > -1, labels_df.columns))
    dff = labels_df[diagnosis]
    with warnings.catch_warnings():
        warnings.simplefilter(action='ignore', category=FutureWarning)
        value_counts = dff.apply(pd.value_counts).fillna(0).T
    value_counts.plot(kind='bar', stacked=True, ax=axis[idx])
    axis[idx].set_title(f'{d} distribution')

df_coor = pd.read_csv('/content/train_label_coordinates.csv')

df_coor.head()

# def display_coor_on_img(c, i, title):
#     center_coordinates = (int(c['x']), int(c['y']))
#     radius = 10
#     color = (255, 0, 0)  # Red color in BGR
#     thickness = 2
#     IMG = i['dicom'].pixel_array
#     IMG_normalized = cv2.normalize(IMG, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)

#     IMG_with_circle = cv2.circle(IMG_normalized.copy(), center_coordinates, radius, color, thickness)

#     # Convert the image from BGR to RGB for correct color display in matplotlib
#     IMG_with_circle = cv2.cvtColor(IMG_with_circle, cv2.COLOR_BGR2RGB)

#     # Display the image
#     plt.imshow(IMG_with_circle)
#     plt.axis('off')  # Turn off axis numbers and ticks
#     plt.title(title)
#     plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder

# Load the CSV file (replace with the actual path)
labels_df = pd.read_csv('/content/train.csv')

# Display the column names in the dataset
print(labels_df.columns)

from sklearn.preprocessing import LabelEncoder

# Identify the columns that need encoding (columns with string data)
categorical_columns = [
    'spinal_canal_stenosis_l1_l2', 'spinal_canal_stenosis_l2_l3',
    'spinal_canal_stenosis_l3_l4', 'spinal_canal_stenosis_l4_l5',
    'spinal_canal_stenosis_l5_s1', 'left_neural_foraminal_narrowing_l1_l2',
    'left_neural_foraminal_narrowing_l2_l3', 'left_neural_foraminal_narrowing_l3_l4',
    'left_neural_foraminal_narrowing_l4_l5', 'left_neural_foraminal_narrowing_l5_s1',
    'right_neural_foraminal_narrowing_l1_l2', 'right_neural_foraminal_narrowing_l2_l3',
    'right_neural_foraminal_narrowing_l3_l4', 'right_neural_foraminal_narrowing_l4_l5',
    'right_neural_foraminal_narrowing_l5_s1', 'left_subarticular_stenosis_l1_l2',
    'left_subarticular_stenosis_l2_l3', 'left_subarticular_stenosis_l3_l4',
    'left_subarticular_stenosis_l4_l5', 'left_subarticular_stenosis_l5_s1',
    'right_subarticular_stenosis_l1_l2', 'right_subarticular_stenosis_l2_l3',
    'right_subarticular_stenosis_l3_l4', 'right_subarticular_stenosis_l4_l5',
    'right_subarticular_stenosis_l5_s1'
]

# Apply label encoding to each of the categorical columns
label_encoder = LabelEncoder()

for col in categorical_columns:
    labels_df[col] = label_encoder.fit_transform(labels_df[col])

# Compute the correlation matrix after encoding
correlation_matrix = labels_df.corr()

# Create the heatmap
plt.figure(figsize=(15, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Feature Correlation Heatmap')
plt.show()

# Handling missing values (if applicable)
print("Missing values before handling:")
print(labels_df.isnull().sum())

# Fill missing values with the mode (most frequent value) for each column
labels_df.fillna(labels_df.mode().iloc[0], inplace=True)

print("Missing values after handling:")
print(labels_df.isnull().sum())

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder

# Load your dataset (assuming it's already loaded into df)
# Drop the 'study_id' column, as it is not a feature
X = labels_df.drop(columns=['study_id', 'spinal_canal_stenosis_l1_l2'])  # Replace with the correct target column
y = labels_df['spinal_canal_stenosis_l1_l2']  # Target column

# Encode the categorical target variable
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Encode the categorical features (if any)
X_encoded = X.apply(lambda col: label_encoder.fit_transform(col) if col.dtype == 'object' else col)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42)

# We can use SMOTE to reduce the class imbalance.
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Initialize the model
model = RandomForestClassifier(class_weight='balanced')

# Fit the model to the training data
model.fit(X_train, y_train)

# Get feature importances
feature_importances = model.feature_importances_

# Print the feature importances
print("Feature Importances:", feature_importances)

# Plot the feature importances
plt.figure(figsize=(10, 6))
plt.barh(X.columns, feature_importances, color='skyblue')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importances from Random Forest')
plt.show()

# from sklearn.preprocessing import StandardScaler

# # Example: Standardizing the dataset
# scaler = StandardScaler()
# X_train_scaled = scaler.fit_transform(X_train)
# X_test_scaled = scaler.transform(X_test)

# from sklearn.decomposition import PCA

# # Example: Applying PCA to reduce dimensions
# pca = PCA(n_components=5)
# X_train_pca = pca.fit_transform(X_train_scaled)
# X_test_pca = pca.transform(X_test_scaled)

# Check for missing values
print(labels_df.isnull().sum())

# Fill missing values with median (or any other suitable strategy)
labels_df.fillna(labels_df.mode(), inplace=True)

# from sklearn.feature_selection import SelectFromModel

# # Apply label encoding to categorical columns
# categorical_columns = [
#     'spinal_canal_stenosis_l1_l2', 'spinal_canal_stenosis_l2_l3',
#     'spinal_canal_stenosis_l3_l4', 'spinal_canal_stenosis_l4_l5',
#     'spinal_canal_stenosis_l5_s1', 'left_neural_foraminal_narrowing_l1_l2',
#     'left_neural_foraminal_narrowing_l2_l3', 'left_neural_foraminal_narrowing_l3_l4',
#     'left_neural_foraminal_narrowing_l4_l5', 'left_neural_foraminal_narrowing_l5_s1',
#     'right_neural_foraminal_narrowing_l1_l2', 'right_neural_foraminal_narrowing_l2_l3',
#     'right_neural_foraminal_narrowing_l3_l4', 'right_neural_foraminal_narrowing_l4_l5',
#     'right_neural_foraminal_narrowing_l5_s1', 'left_subarticular_stenosis_l1_l2',
#     'left_subarticular_stenosis_l2_l3', 'left_subarticular_stenosis_l3_l4',
#     'left_subarticular_stenosis_l4_l5', 'left_subarticular_stenosis_l5_s1',
#     'right_subarticular_stenosis_l1_l2', 'right_subarticular_stenosis_l2_l3',
#     'right_subarticular_stenosis_l3_l4', 'right_subarticular_stenosis_l4_l5',
#     'right_subarticular_stenosis_l5_s1'
# ]

# label_encoder = LabelEncoder()
# for col in categorical_columns:
#     labels_df[col] = label_encoder.fit_transform(labels_df[col])

# # Feature selection using Random Forest
# X = labels_df.drop(['study_id'], axis=1)  # Exclude the study_id column
# y = labels_df['spinal_canal_stenosis_l1_l2']  # Replace with actual target column

# model = RandomForestClassifier(class_weight='balanced')
# model.fit(X, y)

# selected_features = SelectFromModel(model, threshold=0.05).fit_transform(X, y)

# # Standardize the data
# scaler = StandardScaler()
# X_scaled = scaler.fit_transform(X)

# # Visualize transformations
# plt.figure(figsize=(12, 6))
# plt.subplot(1, 2, 1)
# plt.hist(X['spinal_canal_stenosis_l1_l2'], bins=30, color='blue', alpha=0.7)
# plt.title('Original Feature Distribution')

# plt.subplot(1, 2, 2)
# plt.hist(X_scaled[:, 0], bins=30, color='green', alpha=0.7)
# plt.title('Standardized Feature Distribution')
# plt.show()

# # Example of feature engineering by creating a new feature
# labels_df['average_stenosis'] = labels_df[['spinal_canal_stenosis_l1_l2',
#                                          'spinal_canal_stenosis_l2_l3',
#                                          'spinal_canal_stenosis_l3_l4']].mean(axis=1)

# # Apply PCA for dimensionality reduction
# pca = PCA(n_components=5)
# X_pca = pca.fit_transform(X_scaled)

# # Explained variance ratio
# print("Explained variance by principal components:", pca.explained_variance_ratio_)

# # Visualize PCA results
# plt.figure(figsize=(10, 6))
# plt.bar(range(1, 6), pca.explained_variance_ratio_, alpha=0.7, color='red')
# plt.xlabel('Principal Components')
# plt.ylabel('Variance Explained')
# plt.title('PCA Explained Variance')
# plt.show()

# # Now proceed with model building using selected features and transformed data

!pip install prince

# # Fill missing values with the most frequent value (mode) in each column
# # First, handle cases where the mode might not be calculable
# for col in categorical_data.columns:
#     if categorical_data[col].mode().empty:
#         # If mode can't be calculated, fill with a placeholder value
#         categorical_data[col].fillna("Unknown", inplace=True)
#     else:
#         # Otherwise, fill with the mode
#         categorical_data[col].fillna(categorical_data[col].mode().iloc[0], inplace=True)

# # Ensure all columns are of type 'category'
# for col in categorical_data.columns:
#     categorical_data[col] = categorical_data[col].astype('category')

# import pandas as pd
# from prince import MCA

# # Assuming labels_df is your DataFrame
# # Selecting only object type columns for MCA (categorical data)
# categorical_data = labels_df.select_dtypes(include=['object']).copy()

# # Check and handle missing values
# for col in categorical_data.columns:
#     if categorical_data[col].mode().empty:
#         categorical_data[col].fillna("Unknown", inplace=True)
#     else:
#         categorical_data[col].fillna(categorical_data[col].mode().iloc[0], inplace=True)

# # Convert to categorical if not already
# for col in categorical_data.columns:
#     categorical_data[col] = categorical_data[col].astype('category')

# # Now apply MCA
# mca = MCA(n_components=2)
# try:
#     mca_fit = mca.fit(categorical_data)
#     mca_components = mca_fit.transform(categorical_data)
#     print(mca_components)
# except ValueError as e:
#     print(f"An error occurred: {e}")