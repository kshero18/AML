# -*- coding: utf-8 -*-
"""Spine_Degeneration_Study.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CQCI1ucJ-olBOuYkPSKdWfeeV0PDcL5e
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'rsna-2024-lumbar-spine-degenerative-classification:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F71549%2F8561470%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240908%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240908T070721Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7e2a46fe9f13a07eb738a8aeac318d6f96b635e7a1b0b54f99efe2539b75eaba02f2453d816af3781dfd9ac49f6511e27bcaff15630a5ff686f6c0130463733905f5f259adaac5fb5164e8936509425ad9f1b9ed12a71e1210fe3bb17a0925b1c1e814c8baf494b7c6fb0492c1634daf7338f01ad8cea4412f924707aa700a794745fe2ab758a738d90445343630ed39f3df1e30380773a6d6adebd14c2ae129ef779c62e4d1a87088a89b6cb9aa66fda30afcfd1cab920ebd1a5cbba7711d9983c360757ac22d263d23ea12b07caa2a9a8fca40454150ec933a56c08fbe715056f060e8eabb2cc44433158ad4da60b65b73cf61810a6ff9692da109be58e4fd'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

## naming the new dataframes of the csv files

trainset1 = pd.read_csv('/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train.csv')
trainset2 = pd.read_csv('/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_label_coordinates.csv')
trainset3 = pd.read_csv('/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_series_descriptions.csv')

trainset1.head()

trainset2.head()

trainset3.head()

merged_df = pd.merge(trainset2, trainset3, on='series_id', how='inner')
# Assuming 'study_id' is identical in both DataFrames
merged_df['study_id'] = merged_df['study_id_x']  # or merged_df['study_id_y'], depending on preference

# Drop the redundant 'study_id_x' and 'study_id_y' columns
merged_df = merged_df.drop(columns=['study_id_x', 'study_id_y'])

merged_df.head(10)

final_merged_df = pd.merge(merged_df, trainset1, on='study_id', how='inner')
final_merged_df.head(10)

final_merged_df.columns.tolist()

len(final_merged_df)

"""## Image processing

for brightening the image, I am using the loagrithmic transformation
"""

!pip install pydicom

import os
import pydicom
import numpy as np
import matplotlib.pyplot as plt
import cv2

# Function to map the level to the corresponding stenosis column in the DataFrame
def get_stenosis_column(level, condition):
    """
    Returns the appropriate stenosis column based on the level and condition.
    """
    stenosis_map = {
        'L1/L2': {
            'Spinal Canal Stenosis': 'spinal_canal_stenosis_l1_l2',
            'Left Neural Foraminal Narrowing': 'left_neural_foraminal_narrowing_l1_l2',
            'Right Neural Foraminal Narrowing': 'right_neural_foraminal_narrowing_l1_l2',
            'Left Subarticular Stenosis': 'left_subarticular_stenosis_l1_l2',
            'Right Subarticular Stenosis': 'right_subarticular_stenosis_l1_l2',
        },
        'L2/L3': {
            'Spinal Canal Stenosis': 'spinal_canal_stenosis_l2_l3',
            'Left Neural Foraminal Narrowing': 'left_neural_foraminal_narrowing_l2_l3',
            'Right Neural Foraminal Narrowing': 'right_neural_foraminal_narrowing_l2_l3',
            'Left Subarticular Stenosis': 'left_subarticular_stenosis_l2_l3',
            'Right Subarticular Stenosis': 'right_subarticular_stenosis_l2_l3',
        },
        'L3/L4': {
            'Spinal Canal Stenosis': 'spinal_canal_stenosis_l3_l4',
            'Left Neural Foraminal Narrowing': 'left_neural_foraminal_narrowing_l3_l4',
            'Right Neural Foraminal Narrowing': 'right_neural_foraminal_narrowing_l3_l4',
            'Left Subarticular Stenosis': 'left_subarticular_stenosis_l3_l4',
            'Right Subarticular Stenosis': 'right_subarticular_stenosis_l3_l4',
        },
        'L4/L5': {
            'Spinal Canal Stenosis': 'spinal_canal_stenosis_l4_l5',
            'Left Neural Foraminal Narrowing': 'left_neural_foraminal_narrowing_l4_l5',
            'Right Neural Foraminal Narrowing': 'right_neural_foraminal_narrowing_l4_l5',
            'Left Subarticular Stenosis': 'left_subarticular_stenosis_l4_l5',
            'Right Subarticular Stenosis': 'right_subarticular_stenosis_l4_l5',
        },
        'L5/S1': {
            'Spinal Canal Stenosis': 'spinal_canal_stenosis_l5_s1',
            'Left Neural Foraminal Narrowing': 'left_neural_foraminal_narrowing_l5_s1',
            'Right Neural Foraminal Narrowing': 'right_neural_foraminal_narrowing_l5_s1',
            'Left Subarticular Stenosis': 'left_subarticular_stenosis_l5_s1',
            'Right Subarticular Stenosis': 'right_subarticular_stenosis_l5_s1',
        }
    }

    return stenosis_map.get(level, {}).get(condition, None)

# Function to apply CLAHE to the image
def apply_clahe(image):
    """
    Apply CLAHE (Contrast Limited Adaptive Histogram Equalization) to the input image.

    Parameters:
    - image: The input image (assumed to be grayscale).

    Returns:
    - enhanced_image: The image after applying CLAHE.
    """
    # Create a CLAHE object with a clip limit
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    enhanced_image = clahe.apply(image)
    return enhanced_image

# Function to normalize the image data to 8-bit format (uint8)
def normalize_to_uint8(image):
    """
    Normalize the image data to 8-bit format for compatibility with OpenCV and matplotlib.
    """
    # Normalize the image to the range [0, 255]
    image_normalized = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX)
    # Convert to uint8
    return image_normalized.astype(np.uint8)

# Function to color code the severity and normalize it for matplotlib
def get_circle_color(severity):
    """
    Returns the normalized color for the circle based on the severity of the condition.
    Normalizes color values to [0, 1] for matplotlib.
    """
    severity_color_map = {
        'Normal/Mild': (0, 1.0, 0),    # Green (normalized)
        'Moderate': (1.0, 1.0, 0),     # Yellow (normalized)
        'Severe': (1.0, 0, 0)          # Red (normalized)
    }
    return severity_color_map.get(severity, (1.0, 1.0, 1.0))  # Default to white if unknown

# Function to display images grouped by description with hollow dots for severity
def display_images_grouped_by_description(images_by_description, study_id):
    """
    Displays images grouped by description and includes hollow circles representing severity.

    Parameters:
    - images_by_description: Dictionary containing lists of images categorized by description.
    - study_id: The study ID being processed.
    """
    for description, images in images_by_description.items():
        # Create a plot for each description
        num_images = len(images)
        fig, axs = plt.subplots(1, num_images, figsize=(20, 6))

        if num_images == 1:
            axs = [axs]  # To ensure axs is iterable even for a single image

        # Plot each image with a hollow circle for severity
        for i, (img, instance_number, x, y, severity, series_id) in enumerate(images):
            # Apply CLAHE to enhance the image contrast
            enhanced_img = apply_clahe(img)

            # Display the enhanced image with gray color map
            axs[i].imshow(enhanced_img, cmap='gray')
            axs[i].scatter([x], [y], facecolors='none', edgecolors=get_circle_color(severity), s=100, linewidths=2)
            axs[i].set_title(f"{instance_number}.dcm")
            axs[i].axis('off')

        plt.suptitle(f"Study ID: {study_id} | Series ID: {series_id} | Description: {description}", fontsize=16)
        plt.tight_layout()
        plt.show()

# Main function to process images and categorize them by description
def process_study_ids_by_description(df, root_dir):
    """
    Loop through multiple study_ids and categorize the images by description, displaying them with hollow dots for severity.

    Parameters:
    - df: The DataFrame containing all the data.
    - root_dir: The root directory where the DICOM images are stored.
    """
    # Loop through each unique study_id
    for study_id in df['study_id'].unique():
        # Filter the DataFrame for the current study_id
        study_df = df[df['study_id'] == study_id]

        # Initialize a dictionary to categorize images by description
        images_by_description = {}

        # Loop through each unique series_id within the current study_id
        for series_id in study_df['series_id'].unique():
            series_df = study_df[study_df['series_id'] == series_id]
            series_description = series_df['series_description'].iloc[0]  # Get description for the series

            # Path to the study and series folders
            study_folder = os.path.join(root_dir, str(study_id))
            series_folder = os.path.join(study_folder, str(series_id))

            # Check if the folder exists
            if not os.path.exists(series_folder):
                print(f"Series folder {series_folder} not found.")
                continue

            # Loop through the rows and categorize images by description
            for index, row in series_df.iterrows():
                instance_number = int(row['instance_number'])  # Instance number is the DICOM file number
                x = row['x']  # X coordinate
                y = row['y']  # Y coordinate
                level = row['level']  # Level (e.g., L1/L2)
                condition = row['condition']  # Condition (e.g., Spinal Canal Stenosis)

                # Get the appropriate stenosis column based on the level and condition
                stenosis_column = get_stenosis_column(level, condition)
                if stenosis_column is None:
                    print(f"No corresponding stenosis column found for level {level} and condition {condition}.")
                    continue

                # Retrieve the condition status (e.g., "Normal/Mild", "Moderate", "Severe")
                condition_status = row[stenosis_column]

                # Locate the corresponding DICOM file (using the instance number)
                dcm_file = f"{instance_number}.dcm"
                dcm_path = os.path.join(series_folder, dcm_file)

                if not os.path.exists(dcm_path):
                    print(f"DICOM file {dcm_file} not found for Series ID: {series_id}.")
                    continue

                # Load the DICOM file
                dicom_data = pydicom.dcmread(dcm_path)

                # Extract pixel data and normalize to uint8
                image = normalize_to_uint8(dicom_data.pixel_array)

                # Add the image, instance number, and coordinates to the appropriate description category
                if series_description not in images_by_description:
                    images_by_description[series_description] = []
                images_by_description[series_description].append((image, instance_number, x, y, condition_status, series_id))

        # Display the images grouped by description
        display_images_grouped_by_description(images_by_description, study_id)

# Example usage:
root_dir = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images'
process_study_ids_by_description(final_merged_df, root_dir)

import os
import pydicom
import numpy as np
import cv2
from collections import defaultdict

# Function to extract and print dimensions in real time
def extract_and_print_dicom_dimensions(df, root_dir):
    """
    Extract DICOM image dimensions in real time and print them as they are processed.

    Parameters:
    - df: The DataFrame containing all the data.
    - root_dir: The root directory where the DICOM images are stored.
    """
    # Dictionary to group dimensions by description
    dimensions_by_description = defaultdict(list)

    # Loop through each unique study_id
    for study_id in df['study_id'].unique():
        # Filter the DataFrame for the current study_id
        study_df = df[df['study_id'] == study_id]

        # Loop through each unique series_id within the current study_id
        for series_id in study_df['series_id'].unique():
            series_df = study_df[study_df['series_id'] == series_id]
            series_description = series_df['series_description'].iloc[0]  # Get description for the series

            # Path to the study and series folders
            study_folder = os.path.join(root_dir, str(study_id))
            series_folder = os.path.join(study_folder, str(series_id))

            # Check if the folder exists
            if not os.path.exists(series_folder):
                print(f"Series folder {series_folder} not found.")
                continue

            # Loop through the rows and extract dimensions
            for index, row in series_df.iterrows():
                instance_number = int(row['instance_number'])  # Instance number is the DICOM file number

                # Locate the corresponding DICOM file (using the instance number)
                dcm_file = f"{instance_number}.dcm"
                dcm_path = os.path.join(series_folder, dcm_file)

                if not os.path.exists(dcm_path):
                    print(f"DICOM file {dcm_file} not found for Series ID: {series_id}.")
                    continue

                # Load the DICOM file
                dicom_data = pydicom.dcmread(dcm_path)

                # Extract the dimensions (height, width)
                height, width = dicom_data.pixel_array.shape

                # Add dimensions to the description group
                dimensions_by_description[series_description].append((width, height))

                # Print dimensions in real time
                print(f"Study ID: {study_id}, Series ID: {series_id}, Description: {series_description}, "
                      f"Instance: {instance_number}, Width: {width}, Height: {height}")

    return dimensions_by_description

# Example usage:
root_dir = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images'

# Extract and print dimensions in real time
dimensions_by_description = extract_and_print_dicom_dimensions(final_merged_df, root_dir)

import os
import pydicom
import numpy as np
from collections import defaultdict

# Function to extract and print unique dimensions
def extract_unique_dicom_dimensions(df, root_dir):
    """
    Extract and print unique DICOM image dimensions grouped by description.

    Parameters:
    - df: The DataFrame containing all the data.
    - root_dir: The root directory where the DICOM images are stored.

    Returns:
    - unique_dimensions_by_description: Dictionary with unique dimensions grouped by description.
    """
    # Dictionary to store unique dimensions for each description
    unique_dimensions_by_description = defaultdict(set)

    # Loop through each unique study_id
    for study_id in df['study_id'].unique():
        # Filter the DataFrame for the current study_id
        study_df = df[df['study_id'] == study_id]

        # Loop through each unique series_id within the current study_id
        for series_id in study_df['series_id'].unique():
            series_df = study_df[study_df['series_id'] == series_id]
            series_description = series_df['series_description'].iloc[0]  # Get description for the series

            # Path to the study and series folders
            study_folder = os.path.join(root_dir, str(study_id))
            series_folder = os.path.join(study_folder, str(series_id))

            # Check if the folder exists
            if not os.path.exists(series_folder):
                print(f"Series folder {series_folder} not found.")
                continue

            # Loop through the rows and extract dimensions
            for index, row in series_df.iterrows():
                instance_number = int(row['instance_number'])  # Instance number is the DICOM file number

                # Locate the corresponding DICOM file (using the instance number)
                dcm_file = f"{instance_number}.dcm"
                dcm_path = os.path.join(series_folder, dcm_file)

                if not os.path.exists(dcm_path):
                    print(f"DICOM file {dcm_file} not found for Series ID: {series_id}.")
                    continue

                # Load the DICOM file
                dicom_data = pydicom.dcmread(dcm_path)

                # Extract the dimensions (height, width)
                height, width = dicom_data.pixel_array.shape

                # Add the dimensions as a tuple to the set for this description
                unique_dimensions_by_description[series_description].add((width, height))

    # Convert sets to lists and print the unique dimensions
    for description, dimensions in unique_dimensions_by_description.items():
        print(f"\nDescription: {description}")
        print(f"Unique dimensions:")
        for dim in dimensions:
            print(f"Width: {dim[0]}, Height: {dim[1]}")

    return unique_dimensions_by_description

# Example usage:
root_dir = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images'

# Extract and print unique dimensions
unique_dimensions_by_description = extract_unique_dicom_dimensions(final_merged_df, root_dir)

"""## Object Detection Methdod"""

!pip install pylibjpeg pylibjpeg-libjpeg pylibjpeg-openjpeg

import os
import pydicom
import numpy as np
import cv2
from tensorflow.keras.utils import Sequence
from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Input, concatenate, Dropout
from tensorflow.keras.models import Model

# Filter the DataFrame for the 'Sagittal T1' description
sagittal_t1_df = final_merged_df[final_merged_df['series_description'] == 'Sagittal T1'].reset_index(drop=True)

# Function to map the level to the corresponding stenosis column in the DataFrame
def get_stenosis_column(level, condition):
    """
    Returns the appropriate stenosis column based on the level and condition.
    """
    stenosis_map = {
        'L1/L2': {
            'Spinal Canal Stenosis': 'spinal_canal_stenosis_l1_l2',
            'Left Neural Foraminal Narrowing': 'left_neural_foraminal_narrowing_l1_l2',
            'Right Neural Foraminal Narrowing': 'right_neural_foraminal_narrowing_l1_l2',
            'Left Subarticular Stenosis': 'left_subarticular_stenosis_l1_l2',
            'Right Subarticular Stenosis': 'right_subarticular_stenosis_l1_l2',
        },
        'L2/L3': {
            'Spinal Canal Stenosis': 'spinal_canal_stenosis_l2_l3',
            'Left Neural Foraminal Narrowing': 'left_neural_foraminal_narrowing_l2_l3',
            'Right Neural Foraminal Narrowing': 'right_neural_foraminal_narrowing_l2_l3',
            'Left Subarticular Stenosis': 'left_subarticular_stenosis_l2_l3',
            'Right Subarticular Stenosis': 'right_subarticular_stenosis_l2_l3',
        },
        'L3/L4': {
            'Spinal Canal Stenosis': 'spinal_canal_stenosis_l3_l4',
            'Left Neural Foraminal Narrowing': 'left_neural_foraminal_narrowing_l3_l4',
            'Right Neural Foraminal Narrowing': 'right_neural_foraminal_narrowing_l3_l4',
            'Left Subarticular Stenosis': 'left_subarticular_stenosis_l3_l4',
            'Right Subarticular Stenosis': 'right_subarticular_stenosis_l3_l4',
        },
        'L4/L5': {
            'Spinal Canal Stenosis': 'spinal_canal_stenosis_l4_l5',
            'Left Neural Foraminal Narrowing': 'left_neural_foraminal_narrowing_l4_l5',
            'Right Neural Foraminal Narrowing': 'right_neural_foraminal_narrowing_l4_l5',
            'Left Subarticular Stenosis': 'left_subarticular_stenosis_l4_l5',
            'Right Subarticular Stenosis': 'right_subarticular_stenosis_l4_l5',
        },
        'L5/S1': {
            'Spinal Canal Stenosis': 'spinal_canal_stenosis_l5_s1',
            'Left Neural Foraminal Narrowing': 'left_neural_foraminal_narrowing_l5_s1',
            'Right Neural Foraminal Narrowing': 'right_neural_foraminal_narrowing_l5_s1',
            'Left Subarticular Stenosis': 'left_subarticular_stenosis_l5_s1',
            'Right Subarticular Stenosis': 'right_subarticular_stenosis_l5_s1',
        }
    }

    return stenosis_map.get(level, {}).get(condition, None)

# Data Generator class with fallback resizing
class DataGenerator(Sequence):
    def __init__(self, df, root_dir, batch_size=32, shuffle=True):
        self.df = df.reset_index(drop=True)
        self.root_dir = root_dir
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.indexes = np.arange(len(self.df))
        self.on_epoch_end()

    def __len__(self):
        return int(np.floor(len(self.df) / self.batch_size))

    def __getitem__(self, index):
        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]
        batch_df = self.df.iloc[batch_indexes]

        images, conditions, coords = [], [], []
        for _, row in batch_df.iterrows():
            img, original_width, original_height = self.load_and_preprocess_image(
                study_id=row['study_id'],
                series_id=row['series_id'],
                instance_number=int(row['instance_number'])
            )
            if img is None:
                continue

            # Retrieve severity from the correct stenosis column
            stenosis_column = get_stenosis_column(row['level'], row['condition'])
            if stenosis_column is None:
                continue  # Skip if mapping not found

            severity = row[stenosis_column]

            # Get condition and coordinates
            condition = self.encode_condition(severity)
            if condition == -1:
                continue  # Skip if condition is unknown

            # Resize image if necessary (fallback to a fixed size if images have inconsistent shapes)
            if img.shape != (256, 256, 1):
                img = self.resize_image(img, target_size=(256, 256))

            images.append(img)
            conditions.append(condition)
            coords.append([row['x'], row['y']])

        if len(images) == 0:
            raise ValueError(f"Batch {index} is empty. No valid data was loaded.")

        X_images = np.array(images, dtype=np.float32)
        y_conditions = np.array(conditions, dtype=np.int32)
        X_coords = np.array(coords, dtype=np.float32).reshape((-1, 2))

        return {'image_input': X_images, 'coords_input': X_coords}, y_conditions

    def load_and_preprocess_image(self, study_id, series_id, instance_number):
        study_folder = os.path.join(self.root_dir, str(study_id))
        series_folder = os.path.join(study_folder, str(series_id))
        if not os.path.exists(series_folder):
            print(f"Series folder not found: {series_folder}")
            return None

        dcm_file = f"{instance_number}.dcm"
        dcm_path = os.path.join(series_folder, dcm_file)

        if not os.path.exists(dcm_path):
            print(f"DICOM file not found: {dcm_file} in {series_folder}")
            return None

        try:
            dicom_data = pydicom.dcmread(dcm_path)
        except Exception as e:
            print(f"Error reading DICOM file: {dcm_path}. Error: {str(e)}")
            return None

        try:
            image = dicom_data.pixel_array
            image_preprocessed = self.normalize_to_uint8(image)

            # Ensure the image has a single channel (grayscale)
            if len(image_preprocessed.shape) == 2:
                image_preprocessed = np.expand_dims(image_preprocessed, axis=-1)  # Add a channel dimension

            return image_preprocessed, image.shape[1], image.shape[0]

        except Exception as e:
            print(f"Error processing DICOM file: {dcm_path}. Error: {str(e)}")
            return None

    def resize_image(self, image, target_size=(256, 256)):
        """
        Resize the image to a fixed size.
        """
        resized_image = cv2.resize(image, target_size, interpolation=cv2.INTER_AREA)
        return np.expand_dims(resized_image, axis=-1)  # Make sure the image has a single channel

    def normalize_to_uint8(self, image):
        image_normalized = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX)
        return image_normalized.astype(np.uint8)

    def encode_condition(self, severity):
        severity_map = {
            'Normal/Mild': 0,
            'Moderate': 1,
            'Severe': 2
        }
        return severity_map.get(severity, -1)

    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.indexes)

# CNN Model with Global Average Pooling
def create_cnn_with_global_pooling():
    """
    Create a CNN model with Global Average Pooling to handle varying image sizes.
    """
    # Image branch (input can have variable width and height)
    image_input = Input(shape=(None, None, 1), name='image_input')

    # Convolutional layers
    x = Conv2D(32, (3, 3), activation='relu')(image_input)
    x = MaxPooling2D((2, 2))(x)
    x = Conv2D(64, (3, 3), activation='relu')(x)
    x = MaxPooling2D((2, 2))(x)
    x = Conv2D(128, (3, 3), activation='relu')(x)

    # Global Average Pooling layer
    x = GlobalAveragePooling2D()(x)

    # Coordinates branch (input size fixed to 2)
    coords_input = Input(shape=(2,), name='coords_input')
    coords_branch = Dense(32, activation='relu')(coords_input)

    # Combine the image and coordinates branches
    combined = concatenate([x, coords_branch])

    # Fully connected layers
    combined = Dense(256, activation='relu')(combined)
    combined = Dropout(0.5)(combined)
    output = Dense(3, activation='softmax')(combined)  # 3 output classes (Normal/Mild, Moderate, Severe)

    # Define the model
    model = Model(inputs=[image_input, coords_input], outputs=output)

    # Compile the model
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    return model

# Path to DICOM images
root_dir = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images'

# Create the model
model = create_cnn_with_global_pooling()

# Summary of the model
model.summary()

# Train the model using only the 'Sagittal T1' data
epochs = 10
train_generator = DataGenerator(sagittal_t1_df, root_dir, batch_size=16, shuffle=True)

model.fit(train_generator, epochs=epochs)













